# -*- coding: utf-8 -*-
"""CSE 437 project on 'Hate Speech and Offensive Language Detection from Twitter Dataset'

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VPX-WY7CJg6qo5MvIqArKpurxp4ewUWP

# Hate Speech and Offensive Language Detection from Twitter Dataset
"""

pip install lime

import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier as KNN
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, recall_score, precision_score, f1_score
from sklearn.feature_extraction.text import CountVectorizer
import seaborn as sns
from imblearn.over_sampling import RandomOverSampler
import nltk
from nltk.corpus import wordnet
from nltk.stem import WordNetLemmatizer
nltk.download('stopwords')
nltk.download('wordnet')
stopwords = nltk.corpus.stopwords.words('english')
import string
import re
from collections import Counter

import lime
from lime import lime_tabular
from sklearn.pipeline import make_pipeline
import numpy as np

# from google.colab import drive
# drive.mount('/content/gdrive')

"""### Load Dataset and drop unnecessary columns"""

file_link = "https://drive.google.com/file/d/1NuQKNFvSfbx-z8yeuFqVq_cJxgckWkPq/view?usp=sharing"
id = file_link.split("/")[-2]
#print(id)
new_link = f'https://drive.google.com/uc?id={id}'

df = pd.read_csv(new_link)
df=df.drop(df.index[3000:])
df.head()

"""### Visualization of dataset for output(Imbalanced Dataset)"""

flags = ["Hate Speech", "Offensive Language", "Neither"]
values = [df[df["class"] == i].shape[0] for i in range(3)]
plt.bar(flags, values, color="red")
plt.title("Visualization of dataset for output(Imbalanced Dataset)")
print(f"Dataset size: {df.shape}")
plt.show()

"""### Over sampling for this imbalanced dataset"""

ros = RandomOverSampler(random_state=0)
tweet_resampled, class_resampled = ros.fit_resample(np.array(list(df["tweet"])).reshape(-1, 1), list(df["class"]))
tweet_resampled = np.array([tweet[0] for tweet in tweet_resampled])
print(f"Dataset size after oversampling: {len(tweet_resampled)}")

"""### Visualization of dataset after applying oversampling"""

values = [class_resampled.count(i) for i in range(3)]
plt.bar(flags, values, color="yellow")
plt.title("Visualization class distribution for After Applying Oversampling")
plt.show()

"""### Preprocessing
#### Steps
* Removal of Punctuations
* Lower Casing
* Tokenization
* Removal of Stop Words
* Lemmatization


"""

# Punctuation Removal

def remove_punctuation(text):
    punctuation_free = "".join([i for i in text if i not in string.punctuation]).strip()
    return punctuation_free

# Tokenization

def tokenize(text):
    tokens = re.split(r"\s+", text)
    return tokens

# Remove Stopwords

def remove_stopwords(text):
    removed_stopwords = [i for i in text if i not in stopwords]
    return removed_stopwords

# Lemmatization

wordnet_lemmatizer = WordNetLemmatizer()
def lemmatize(text):
    lemmatized_text = [wordnet_lemmatizer.lemmatize(word) for word in text]
    return lemmatized_text

# # Preprocess the resampled text data

tweets_punctuation_free = np.array([remove_punctuation(tweet) for tweet in tweet_resampled])
tweets_lowercase = np.array([tweet.lower() for tweet in tweets_punctuation_free])
tweets_tokenized = np.array([tokenize(tweet) for tweet in tweets_lowercase], dtype=object)
tweets_no_stop_words = np.array([remove_stopwords(tweet) for tweet in tweets_tokenized], dtype=object)
tweets_no_stop_words = np.array([remove_stopwords(tweet) for tweet in tweets_tokenized], dtype=object)
tweets_lemmatized = np.array([lemmatize(tweet) for tweet in tweets_no_stop_words], dtype=object)

"""### Feature Extraction - Bag of Word using Unigrams"""

# Feature Extraction using Count Vectorization

from itertools import count

vectorizer = CountVectorizer()
tweets_lemmatized_str = [" ".join(tweet) for tweet in tweets_lemmatized]
count_vectorized_tweets = vectorizer.fit_transform(tweets_lemmatized_str).toarray()

# Split the dataset between Train and Test Set

X_train, X_test, y_train, y_test = train_test_split(count_vectorized_tweets, class_resampled, test_size=0.2, random_state=42, shuffle=True)

"""### Fit the models
Algorithms
* Logistic Regression
* Naive Bayes Classifier
* Kth Nearest Neighbor

### Logistic Regression
"""

reg = LogisticRegression()
reg.fit(X_train, y_train)
reg_y_predict = reg.predict(X_test)
reg_cf_matrix = confusion_matrix(y_test, reg_y_predict)
reg_train_score = reg.score(X_train, y_train)
reg_test_score = reg.score(X_test, y_test)
print(f"Training accuracy: {reg_train_score}")
print(f"Testing accuracy: {reg_test_score}")

# Create a LIME explainer
explainer = lime_tabular.LimeTabularExplainer(X_train, mode="classification", feature_names=vectorizer.get_feature_names_out())

# Create a function to predict with your logistic regression model
predict_fn = lambda x: reg.predict_proba(x)

# Choose an instance to explain
instance_to_explain = X_test[0]

# Get the explanation for the instance
explanation = explainer.explain_instance(instance_to_explain, predict_fn, num_features=len(instance_to_explain))

# Display the results
explanation.show_in_notebook(show_table=True, show_all=False)

"""

```
`# This is formatted as code`
```

### Naive Bayes Classifier"""

nvc = GaussianNB()
nvc.fit(X_train, y_train)
nvc_y_predict = nvc.predict(X_test)
nvc_cf_matrix = confusion_matrix(y_test, nvc_y_predict)
nvc_train_score = nvc.score(X_train, y_train)
nvc_test_score = nvc.score(X_test, y_test)
print(f"Training accuracy: {nvc_train_score}")
print(f"Testing accuracy: {nvc_test_score}")

# Create a LIME explainer for Gaussian Naive Bayes
nvc_explainer = lime_tabular.LimeTabularExplainer(X_train, mode="classification", feature_names=vectorizer.get_feature_names_out())

# Create a function to predict with your Gaussian Naive Bayes model
nvc_predict_fn = lambda x: nvc.predict_proba(x)

# Choose an instance to explain
nvc_instance_to_explain = X_test[0]

# Get the explanation for the instance
nvc_explanation = nvc_explainer.explain_instance(nvc_instance_to_explain, nvc_predict_fn, num_features=len(nvc_instance_to_explain))

# Display the results
nvc_explanation.show_in_notebook(show_table=True, show_all=False)

"""### Kth Nearest Neighbor"""

knn = KNN(n_neighbors = 3)
knn.fit(X_train, y_train)
knn_y_predict = knn.predict(X_test)
knn_cf_matrix = confusion_matrix(y_test, knn_y_predict)
knn_train_score = knn.score(X_train, y_train)
knn_test_score = knn.score(X_test, y_test)
print(f"Training accuracy: {knn_train_score}")
print(f"Testing accuracy: {knn_test_score}")

# Create a LIME explainer for KNN
knn_explainer = lime_tabular.LimeTabularExplainer(X_train, mode="classification", feature_names=vectorizer.get_feature_names_out())

# Create a function to predict with your KNN model
knn_predict_fn = lambda x: knn.predict_proba(x)

# Choose an instance to explain for KNN
knn_instance_to_explain = X_test[0]

# Get the explanation for the instance
knn_explanation = knn_explainer.explain_instance(knn_instance_to_explain, knn_predict_fn, num_features=len(knn_instance_to_explain))

# Display the results
knn_explanation.show_in_notebook(show_table=True, show_all=False)

"""### Confusion Matrix, Precision, Recall and F1 Score for Each Model"""

# Reference: https://www.stackvidhya.com/plot-confusion-matrix-in-python-and-why/#:~:text=Plot%20Confusion%20Matrix%20for%20Binary%20Classes%20With%20Labels&text=You%20need%20to%20create%20a,matrix%20with%20the%20labels%20annotation.

def display_confusion_matrix(cf_matrix, model_name, color_map):
    dataframe_cfm = pd.DataFrame(cf_matrix, range(3), range(3))
    plt.figure(figsize = (7,5))
    ax = sns.heatmap(dataframe_cfm, annot=True, fmt='', cmap=color_map)
    ax.set_title(f'Confusion Matrix for {model_name} \n\n');
    ax.set_xlabel('\nPredicted Label')
    ax.set_ylabel('Actual Label');
    ax.xaxis.set_ticklabels(['0','1', "2"])
    ax.yaxis.set_ticklabels(['hate_speech','offensive_language', "neither"])
    plt.show()

# Calculate Precision, Recall and F1 Scores

precision_scores = [precision_score(y_test, reg.predict(X_test), average=None), precision_score(y_test, nvc.predict(X_test), average=None), precision_score(y_test, knn.predict(X_test), average=None)]
recall_scores = [recall_score(y_test, reg.predict(X_test), average=None), recall_score(y_test, nvc.predict(X_test), average=None), recall_score(y_test, knn.predict(X_test), average=None)]
f1_scores = [f1_score(y_test, reg.predict(X_test), average=None), f1_score(y_test, nvc.predict(X_test), average=None), f1_score(y_test, knn.predict(X_test), average=None)]

# Confusion Matrix for Logistic Regression Model
print("     precision     recall     f1_score")
for i in range(3):
    print(f"{i}{round(precision_scores[0][i], 2)}{round(recall_scores[0][i], 2)}       {round(f1_scores[0][i], 2)}")

display_confusion_matrix(reg_cf_matrix, "Logistic Regression", "YlGnBu")

# Confusion Matrix for Naive Bayes Classifier Model

print("     precision     recall     f1_score")
for i in range(3):
    print(f"{i}    {round(precision_scores[1][i], 2)}           {round(recall_scores[1][i], 2)}        {round(f1_scores[1][i], 2)}")

display_confusion_matrix(nvc_cf_matrix, "Naive Bayes Classifier", "Greens")

# Confusion Matrix for Kth Nearest Neighbor Model

print("     precision     recall     f1_score")
for i in range(3):
    print(f"{i}    {round(precision_scores[2][i], 2)}         {round(recall_scores[2][i], 2)}        {round(f1_scores[2][i], 2)}" )


display_confusion_matrix(knn_cf_matrix, "Kth Nearest Neighbor", "Blues")

labels = ["LR", "NVC", "KNN"]

train_acc = [reg_train_score*100, nvc_train_score*100, knn_train_score*100]
test_acc = [reg_test_score*100, nvc_test_score*100, knn_test_score*100]

train_acc = np.array(train_acc, dtype=int)
test_acc = np.array(test_acc, dtype=int)

x = np.arange(len(labels))
width = 0.3

fig, ax = plt.subplots(figsize=(10, 5))
rects1 = ax.bar(x - width/2 - 0.05, train_acc, width, label="Training Accuracy")
rects2 = ax.bar(x + width/2 + 0.05, test_acc, width, label="Testing Accuracy")
ax.set_ylabel("Accuracy")
ax.set_xticks(x)
ax.set_xticklabels(labels)
ax.legend(loc = "upper right")
ax.bar_label(rects1)
ax.bar_label(rects2)
fig.tight_layout()
plt.show()